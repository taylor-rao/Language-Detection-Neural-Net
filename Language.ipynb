{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taylor-rao/Language-Detection-Neural-Net/blob/master/Language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13oX863sBO5",
        "colab_type": "text"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "The purpose of this project is to create a neural network that can be feed text and detect which language it's written in. Even though I don't know any Spanish or German, I can distinguish between Spanish and German text just because I've learned to recognize different charactors and charactor patterns. At this point in its evolution, the program can tell the difference between English and Spanish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcbrlNXmKvt0",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mhtlcVnHWRO",
        "colab_type": "code",
        "outputId": "54b3f027-421f-4608-b2e2-54d8be39c76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "!pip install python-docx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184490 sha256=d6113c8bf6d5d4046df3eda2e8a4d420867c7f20bfa602939de7ee0110ab31bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mRUNgu-HWTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import docx\n",
        "from docx import Document\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWw0J6OqKjer",
        "colab_type": "text"
      },
      "source": [
        "##Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T7O1HJgHWWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readtxt(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    fullText = []\n",
        "    for para in doc.paragraphs:\n",
        "        fullText.append(para.text)\n",
        "    return '\\n'.join(fullText)\n",
        "\n",
        "def purity(language):\n",
        "  language = language.replace('\\n', ' ')\n",
        "  language = language.replace('\\'', '')\n",
        "  res = ''.join([i for i in language if not i.isdigit()])\n",
        "  return res\n",
        "\n",
        "\n",
        "def createSamples(string):\n",
        "  subString = len(string)//100\n",
        "  sampleString = []\n",
        "  for i in range(0, subString):\n",
        "    ind= i*100\n",
        "    sampleString.append(string[ind:ind+100])\n",
        "  return(sampleString)\n",
        "\n",
        "def charCount(string):\n",
        "  thisList = []\n",
        "  totalSet = set(string)\n",
        "  for i in totalSet:\n",
        "    n = 0\n",
        "    for j in string:\n",
        "      if i == j:\n",
        "        n += 1\n",
        "    thisList.append((i,n))\n",
        "  sortedList = sorted(thisList, key=lambda tup: tup[1])\n",
        "  return sortedList\n",
        "\n",
        "def cleanse (tup):\n",
        "  myList = []\n",
        "  n = 10000\n",
        "  i = len(myList) - 1\n",
        "  while n > 1000:\n",
        "    myList.append(tup[i][0])\n",
        "    n = tup[i][1]\n",
        "    i -= 1\n",
        "  return(myList)\n",
        "\n",
        "def encoder(string, col):\n",
        "  cask = np.zeros((len(string), len(col)), dtype = int)\n",
        "  for i in range(len(string)):\n",
        "    for j in range(len(col)):\n",
        "      if string[i] == col[j]:\n",
        "        cask[i][j] = 1\n",
        "      else:\n",
        "        cask[i][j] =0\n",
        "  return cask.flatten()\n",
        "\n",
        "def rsamples(l, length):\n",
        "  indices = np.random.choice(len(l), length, replace=False)\n",
        "  myList = []\n",
        "  for i in indices:\n",
        "    myList.append(l[i])\n",
        "  return myList\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HEf1imwP9nR",
        "colab_type": "text"
      },
      "source": [
        "#Data Wrangling\n",
        "\n",
        "These blocks of code break break up the each file into 100 charactor chunks. Before these chunks can be fed into a neural network, they need to converted into numerical form which is done using one-hot encoding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw667un_HWYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spa = str(readtxt('spanish.docx'))\n",
        "eng= str(readtxt('english.docx'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t6AJOutHWbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spa = purity(spa)\n",
        "eng = purity(eng)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlIGvUyvHWdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "engSamples = createSamples(eng)\n",
        "spaSamples = createSamples(spa)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi_5rN7bHWgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total = eng + spa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmSyVO_8qN74",
        "colab_type": "text"
      },
      "source": [
        "Form a list of these 100 charactor substrings and randomize the order in which they appear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joDX_VlcHWtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reng = rsamples(engSamples, 50000)\n",
        "rspa = rsamples(spaSamples, 10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFOeStRQa0kp",
        "colab_type": "text"
      },
      "source": [
        "##Create list of charactors that occur more than 1000 times.\n",
        "\n",
        "In order to shorten the input vectors, the uncommon charactors are left out of the columns for the one-hot encoded chunks. This is done to each 100 charactor chunk using the 'encoder' function defined at the top which takes a string as its first argument and the columns to be encoded onto as its second argument. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWdmgdH-HWjP",
        "colab_type": "code",
        "outputId": "fd54241f-b3df-4ae9-e415-a6a9dc79d4cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "charCount(total)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('æ', 1),\n",
              " ('>', 1),\n",
              " ('<', 2),\n",
              " ('^', 2),\n",
              " ('œ', 3),\n",
              " ('î', 3),\n",
              " ('~', 4),\n",
              " ('À', 4),\n",
              " ('ô', 4),\n",
              " ('Í', 5),\n",
              " ('%', 6),\n",
              " ('ä', 6),\n",
              " ('ç', 7),\n",
              " ('@', 8),\n",
              " ('&', 8),\n",
              " ('â', 12),\n",
              " ('ê', 23),\n",
              " ('à', 28),\n",
              " ('ý', 32),\n",
              " ('©', 35),\n",
              " ('ö', 35),\n",
              " ('Ó', 36),\n",
              " ('ï', 39),\n",
              " ('É', 59),\n",
              " ('+', 89),\n",
              " ('ü', 98),\n",
              " ('Á', 105),\n",
              " ('$', 111),\n",
              " ('¿', 112),\n",
              " ('/', 130),\n",
              " ('¡', 180),\n",
              " ('ë', 240),\n",
              " ('è', 267),\n",
              " ('‘', 268),\n",
              " ('Z', 301),\n",
              " ('Q', 349),\n",
              " ('|', 408),\n",
              " ('[', 418),\n",
              " (']', 421),\n",
              " ('»', 446),\n",
              " ('«', 449),\n",
              " ('Ú', 516),\n",
              " ('*', 716),\n",
              " ('#', 741),\n",
              " ('X', 1271),\n",
              " ('=', 1762),\n",
              " ('ñ', 1935),\n",
              " ('ú', 1978),\n",
              " ('U', 2097),\n",
              " ('—', 2120),\n",
              " ('J', 2145),\n",
              " (')', 2368),\n",
              " ('(', 2369),\n",
              " ('K', 2815),\n",
              " ('V', 3408),\n",
              " (':', 3420),\n",
              " ('Y', 3852),\n",
              " ('é', 4301),\n",
              " ('L', 4424),\n",
              " ('G', 4686),\n",
              " (';', 4953),\n",
              " ('_', 5486),\n",
              " ('O', 5923),\n",
              " ('á', 6151),\n",
              " ('F', 6894),\n",
              " ('D', 7032),\n",
              " ('’', 7260),\n",
              " ('?', 7282),\n",
              " ('!', 8327),\n",
              " ('”', 8970),\n",
              " ('W', 8973),\n",
              " ('“', 9003),\n",
              " ('R', 9102),\n",
              " ('í', 9303),\n",
              " ('E', 9412),\n",
              " ('z', 9458),\n",
              " ('B', 10009),\n",
              " ('C', 10053),\n",
              " ('N', 10403),\n",
              " ('ó', 10505),\n",
              " ('j', 10704),\n",
              " ('M', 10953),\n",
              " ('H', 11754),\n",
              " ('x', 12761),\n",
              " ('S', 12872),\n",
              " ('P', 16245),\n",
              " ('q', 16567),\n",
              " ('-', 19904),\n",
              " ('A', 21630),\n",
              " ('T', 23204),\n",
              " ('I', 24766),\n",
              " ('\"', 26097),\n",
              " ('k', 48656),\n",
              " ('v', 82706),\n",
              " ('.', 95589),\n",
              " ('b', 108736),\n",
              " (',', 128550),\n",
              " ('y', 136780),\n",
              " ('p', 143210),\n",
              " ('w', 146102),\n",
              " ('g', 147226),\n",
              " ('f', 168074),\n",
              " ('m', 195180),\n",
              " ('c', 222670),\n",
              " ('u', 230441),\n",
              " ('l', 331771),\n",
              " ('d', 362444),\n",
              " ('h', 445670),\n",
              " ('r', 490644),\n",
              " ('s', 530307),\n",
              " ('i', 538527),\n",
              " ('n', 583481),\n",
              " ('o', 627944),\n",
              " ('t', 672976),\n",
              " ('a', 685584),\n",
              " ('e', 1012056),\n",
              " (' ', 1871459)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueyuuH24HWl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "usedChars = cleanse(charCount(total))\n",
        "usedChars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9MDmVaFCgpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elist = []\n",
        "for i in reng:\n",
        "  elist.append([encoder(i, usedChars), 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGgw4WGrCgxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slist = []\n",
        "for i in rspa:\n",
        "  slist.append([encoder(i, usedChars), 1])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rauzo1c7Cg17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tlist = elist + slist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg44cvgxCg-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle(tlist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYWJjqmosc1u",
        "colab_type": "text"
      },
      "source": [
        "##Create predictors and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIy2BJu0HFjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.zeros((len(tlist),7400), dtype = int)\n",
        "for i in range(len(tlist)):\n",
        "  data[i] = tlist[i][0]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D23aRjCyChOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = np.zeros(len(tlist), dtype = int)\n",
        "n=0\n",
        "for i in tlist:\n",
        "  labels[n] = tlist[n][1]\n",
        "  n += 1\n",
        "\n",
        "labels = to_categorical(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nFOmgp7-H7S",
        "colab_type": "text"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84uWQZjCChpJ",
        "colab_type": "code",
        "outputId": "772be747-d519-47e1-b6d0-68040d75b236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(32, input_shape=(7400,)))\n",
        "model.add(Dense(32, activation = 'relu'))\n",
        "model.add(Dense(2, activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0803 13:08:24.930870 140612298770304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0803 13:08:24.988192 140612298770304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0803 13:08:24.997697 140612298770304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0Cw59kk-Rhu",
        "colab_type": "text"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt6eU7mkChkC",
        "colab_type": "code",
        "outputId": "a44199b7-9f1e-4d30-e6a4-6a6ab7e7d053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(data, labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0803 13:10:20.453600 140612298770304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0803 13:10:20.513772 140612298770304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 0.0414 - acc: 0.9847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2996eec88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3W2OwLE_oin",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion and next steps\n",
        "\n",
        "This model reached over 98% accuracy on testing data and theres no reason to think that it couldn't get even more accurate if fed more data. The next steps here would be to change the code to make it easy to add other languages. Some basic webscraping could be done to build datasets similar to the ones in spanish and english I used here."
      ]
    }
  ]
}